import os
import sys
import django
import time
import random
import logging
from datetime import datetime, timedelta
from django.utils import timezone
import jdatetime

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('master_bilet_crawler.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# ØªÙ†Ø¸ÛŒÙ… Ù…Ø­ÛŒØ· Django
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'sellei.settings')
django.setup()

from flights.models import Flight, Bus, Train, CrawlerStatus
from flights.utils import get_flights, get_buses, get_trains, get_weather, get_calendar_events

class MasterBiletCrawler:
    def __init__(self):
        self.max_retries = 3
        self.delay_between_attempts = 10
        self.delay_between_days = 15
        
        self.routes = {
            'flights': {'origin': 'THR', 'destination': 'MHD'},
            'buses': {'origin': 'tehran', 'destination': 'mashhad'},
            'trains': {'origin': 'tehran', 'destination': 'mashhad'}
        }

    def get_shamsi_date(self, date):
        jdate = jdatetime.date.fromgregorian(date=date)
        return f"{jdate.year}-{jdate.month:02d}-{jdate.day:02d}"

    def crawl_transport_data(self, date, transport_type):
        shamsi_date = self.get_shamsi_date(date)
        config = {
            'flights': {'name': 'Ù¾Ø±ÙˆØ§Ø²', 'get_func': get_flights},
            'buses': {'name': 'Ø§ØªÙˆØ¨ÙˆØ³', 'get_func': get_buses},
            'trains': {'name': 'Ù‚Ø·Ø§Ø±', 'get_func': get_trains}
        }[transport_type]
        
        route = self.routes[transport_type]
        
        for attempt in range(self.max_retries):
            try:
                logging.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª {config['name']}Ù‡Ø§ Ø§Ø² {route['origin']} Ø¨Ù‡ {route['destination']} Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ® {shamsi_date}")
                results = config['get_func'](shamsi_date, route['origin'], route['destination'])
                
                if results:
                    logging.info(f"âœ… {len(results)} {config['name']} Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯")
                    return results
                else:
                    logging.warning(f"âš ï¸ Ù‡ÛŒÚ† {config['name']}ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                    return []
                    
            except Exception as e:
                if attempt == self.max_retries - 1:
                    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª {config['name']}Ù‡Ø§: {str(e)}")
                else:
                    time.sleep(random.randint(self.delay_between_attempts, self.delay_between_attempts + 5))
        
        return []

    def crawl_for_date(self, date):
        shamsi_date = self.get_shamsi_date(date)
        result = {'flights': [], 'buses': [], 'trains': [], 'weather': None, 'calendar_event': None}
        
        try:
            logging.info(f"\n{'='*50}\nğŸ“… Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ®: {shamsi_date}\n{'='*50}")
            
            # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ‚ÙˆÛŒÙ… Ùˆ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§
            result['calendar_event'] = get_calendar_events(date)
            result['weather'] = get_weather("Ù…Ø´Ù‡Ø¯")
            
            # Ú©Ø±Ø§ÙˆÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            result['flights'] = self.crawl_transport_data(date, 'flights')
            result['buses'] = self.crawl_transport_data(date, 'buses')
            result['trains'] = self.crawl_transport_data(date, 'trains')
            
            # Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¶Ø¹ÛŒØª Ú©Ø±Ø§ÙˆÙ„Ø±
            CrawlerStatus.objects.create(
                crawler_type='all',
                last_run=timezone.now(),
                next_run=timezone.now() + timedelta(hours=6),
                status='completed',
                items_crawled=len(result['flights']) + len(result['buses']) + len(result['trains'])
            )
            
            return result
            
        except Exception as e:
            logging.error(f"ğŸš¨ Ø®Ø·Ø§ Ø¯Ø± Ú©Ø±Ø§ÙˆÙ„ÛŒÙ†Ú¯: {str(e)}")
            return None

def main():
    crawler = MasterBiletCrawler()
    
    try:
        days_to_crawl = 3
        start_date = datetime.now().date()
        
        for i in range(days_to_crawl):
            current_date = start_date + timedelta(days=i)
            result = crawler.crawl_for_date(current_date)
            
            if result:
                shamsi_date = crawler.get_shamsi_date(current_date)
                logging.info(f"\n{'='*50}\nÙ†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ® {shamsi_date}:")
                logging.info(f"- Ù¾Ø±ÙˆØ§Ø²Ù‡Ø§: {len(result['flights'])} Ù…ÙˆØ±Ø¯")
                logging.info(f"- Ø§ØªÙˆØ¨ÙˆØ³â€ŒÙ‡Ø§: {len(result['buses'])} Ù…ÙˆØ±Ø¯")
                logging.info(f"- Ù‚Ø·Ø§Ø±Ù‡Ø§: {len(result['trains'])} Ù…ÙˆØ±Ø¯")
            
            if i < days_to_crawl - 1:
                time.sleep(crawler.delay_between_days)
                
    except KeyboardInterrupt:
        logging.info("\nğŸ›‘ Ø¯Ø±ÛŒØ§ÙØª Ø³ÛŒÚ¯Ù†Ø§Ù„ ØªÙˆÙ‚Ù")
    except Exception as e:
        logging.error(f"\nğŸ’£ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {str(e)}")
    finally:
        logging.info("\nğŸ Ù¾Ø§ÛŒØ§Ù† Ø¹Ù…Ù„ÛŒØ§Øª Ú©Ø±Ø§ÙˆÙ„ÛŒÙ†Ú¯")

if __name__ == "__main__":
    logging.info("\nğŸŒŸ Ø´Ø±ÙˆØ¹ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ø±Ø§ÙˆÙ„Ø± Ù…Ø³ØªØ± Ø¨Ù„ÛŒØ·")
    main()